[
  {
    "path": "posts/2022-02-17-deepparse/",
    "title": "deepparse",
    "description": "Deepparse: a state-of-the-art Python library for parsing multinational street addresses using deep learning.",
    "author": [
      {
        "name": "Marouane Yassine",
        "url": "https://www.linkedin.com/in/marouaneyassine1"
      },
      {
        "name": "David Beauchemin",
        "url": "https://www.linkedin.com/in/david-beauchemin/"
      }
    ],
    "date": "2022-02-17",
    "categories": [],
    "contents": "\nDeepparse\nRecord linkage consists of identifying multiple entries which refer to the same entity within a database or other data sources. Since many real-world entities are at least partially identified by their physical location, data parsing can prove itself to be quite useful in the context of record linkage.\nDeepparse (deepparse.org) is an open-source python package that features state-of-the-art natural language processing models trained to achieve the task of address parsing. Contrary to many existing solutions, deepparse has been created with the objective of efficient multinational address parsing. Therefore, our models have been trained on data from 20 countries with different languages and address formats, and yielded accuracies around 99% when tested. In addition, we have conducted tests to evaluate the degree to which these models can generalize their performance beyond the countries in which they were trained. The results and details can be found in our published paper Leveraging subword embeddings for multinational address parsing.\nIn this post, we are going to cover the four main features of deepparse, namely:\nusing our pre-trained models to parse multinational addresses,\nretraining our models to improve performances on specific countries or address patterns,\nretraining our models using new address components,\naltering the configuration of our models to better fit your use case.\nOut-of-the-box parsing\nWhether the addresses you wish to parse originate from the countries on which deepparse’s models were trained, or whether you wish to experiment with the package’s API, you can easily get started with a few lines of code.\nFirst, you’ll need to choose one of the featured pre-trained models and instantiate an AddressParser. We offer the three following models:\nbpemb: named after the multilingual subword embeddings used to train it, this model yields the best overall performance but suffers from a heavy memory footprint,\nfasttext: this model’s architecture is similar to bpemb’s but uses FastText’s pre-trained french embeddings (http://fasttext.cc/). This model is lighter and faster than the previous one whilst still offering a good performance,\nfasttext-light: this model is identical to the fasttext model, but uses an optimisation technique to make its memory usage even lower.\nIn addition to these models, we offer the possibility of adding an attention mechanism to further enhance performance.\n\nfrom deepparse.parser import AddressParser\n\nparser = AddressParser(model_type=\"bpemb\", attention_mechanism=True, device=0)\n\nIt is also possible to use a GPU to make the models’ predictions faster with the device argument.\nOnce the AddressParser is defined, you can parse an address or a list of addresses in any language with a simple call to the parser.\n\naddreses = [\"서울특별시 종로구 사직로3길 23\", \"777 Brockton Avenue, Abington MA 2351\"]\n\nparsed_addresses = parser(addreses)\n\nfor parsed_address in parsed_addresses:\n  print(parsed_address.address_parsed_components)\n\nOUT:\n\n[('서울특별시', 'Province'), ('종로구', 'Municipality'), ('사직로3길', 'StreetName'), ('23', 'StreetNumber')]\n[('777', 'StreetNumber'), ('Brockton', 'StreetName'), ('Avenue,', 'StreetName'), ('Abington', 'Municipality'), ('MA', 'Province'), ('2351', 'PostalCode')]\n\nFine-tuning models\nAs mentioned before, deepparse’s models have been trained on a select number of countries, which means that the addresses you wish to parse may not have been encountered before during training. This may lead to a lower parsing performance. However, you need not worry as long as you have access to some labelled data for your use case (you can also check out our complete dataset. This is due to the retraining feature that enables fine-tuning of the pre-defined and pre-trained models in order to boost performance for specific use cases.\nRetraining a model is as simple as making a call to an AddressParser’s retrain() method. The retrained model will be of the same type as the one with which the AddressParser was initialized.\n\ntraining_container = PickleDatasetContainer(“PATH TO TRAINING DATA”)\n\naddress_parser = AddressParser(model_type=\"fasttext”)\n\naddress_parser.retrain(training_container, train_ratio=0.8, epochs=100)\n\nMultiple arguments enable you to configure the training process’s hyperparameters, such as the number of training epochs and the batch size.\nFurthermore, if you wish to test your retrained model’s performance, you can use the test() function to compute and return the main accuracy on a test sample.\n\ntesting_container = PickleDatasetContainer(“PATH TO TESTING DATA”)\n\naddress_parser.test(testing_container)\n\nIf you are wondering what the data format should be, you can look at the original training data, which is openly available.\nDefining new address components\nJust like the original models are not enough to cover all use cases, one may also need to update the original parsing labels to better fit their needs. It’s possible (and easy) to do so during the retraining process by specifying a value for the prediction_tags argument of the retrain() function. The tags must be defined in a dictionary of which the keys are the new tags, and the values are their respective indices starting at 0.\nFor instance, let’s suppose we wish to retrain a model to recognize postal boxes, towns and countries. First of all, we would need to define a dictionary with the appropriate tags.\n\ntags = {\"po_box\": 0, \"town\": 1, \"country\": 2, \"EOS\": 3}\n\nNotice the presence of an extra tag (i.e. EOS). This tag must be present in the dictionary for the retraining to function correctly.\nOnce the tags are defined, we simply need to run the retraining process.\n\naddress_parser.retrain(training_container,\n                       train_ratio=0.8,\n                       epochs=100,\n                       batch_size=8,\n                       num_workers=2,\n                       prediction_tags=tag_dictionary,\n                       logging_path=logging_path)\n\nModifying models’ architecture\nFinally, if you are a machine/deep learning practitioner, you might be interested in altering our models’ architecture to experiment with different hyperparameters. All the parsing models are sequence-to-sequence artificial neural networks consisting of an encoder and a decoder, built using LSTMs. While retraining a model, you can easily modify the number of layers in each part of the network and the dimensions of the hidden state using the seq2seq_params argument. Like the tags, new parameters must be defined inside a dictionary.\n\nseq2seq_params = {\n    \"encoder_hidden_size\": 512,\n    \"decoder_hidden_size\": 512\n}\n\naddress_parser.retrain(training_container, \n                       train_ratio=0.8,\n                       epochs=100,\n                       seq2seq_params= seq2seq_params)\n\nConclusion\nWhen address parsing stands in the way of a successful record linkage, deepparse can alleviate some of the task’s complexity by providing a good parsing performance which can be further enhanced using its retraining features.\nWe welcome contributions to the library, as well as questions. So do not hesitate to stop by the Github repository if you have any inquiries!\nAbout the Authors\nMarouane Yassine is a data scientist at Laval University’s Institute Intelligence and Data. With a background in software engineering, he is passionate about deep learning and natural language processing and loves to perform research and build solutions related to those exciting fields.\nDavid Beauchemin trained as an actuary, computer scientist, software engineer and holds an MSc in machine learning. He is currently a Ph.D. student in machine learning. His expertise is at the crossroads of insurance, laws, machine learning, software engineering, and the operationalization of AI systems.\n\n\n\n",
    "preview": "https://raw.githubusercontent.com/GRAAL-Research/deepparse/master/docs/source/_static/logos/deepparse.png",
    "last_modified": "2022-02-18T12:27:08-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-16-youre-invited-linkage-seminar-series-thursday-february-24/",
    "title": "Linkage Seminar Series - Thursday February 24",
    "description": "For February’s Linkage Seminar, Amy O’Hara will be joined by Thais Menezes, a PhD student at SFI Centre for Research Training in Foundations of Data Science, University College Dublin. Thais’ work incorporates the household structure in recording linkage matching in the Ireland census database to make the process of matching individuals easier and more accurate.",
    "author": [
      {
        "name": "RLIG",
        "url": {}
      }
    ],
    "date": "2022-02-16",
    "categories": [],
    "contents": "\nYou are invited\n\n \nMDI and the ASA Record Linkage Interest Group’s Linkage Seminar Series: \nHausdorff Distance: A Powerful Tool to Match Households in Record Linkage\nThursday, February 24, 2022\n1 - 2 PM ET\n Register Here \nZoom Webinar\n\nFor February’s Linkage Seminar, Amy O’Hara will be joined by Thais Menezes, a PhD student at SFI Centre for Research Training in Foundations of Data Science, University College Dublin. Thais’ work incorporates the household structure in recording linkage matching in the Ireland census database to make the process of matching individuals easier and more accurate.\nThis seminar will be recorded and resources will be posted online after the event.\nGuest Speaker: Thais Menezes\n\n\nThais Menezes\nThais Menezes is a Ph.D. student of the SFI Centre for Research Training in Foundations of Data Science (https://www.data-science.ie/) program. She is based in Ireland and her university is the University College Dublin (UCD). Currently, her thesis is regarding Record Linkage and she works with Dt Michael Fop and Professor Brendan Murph - both also from UCD. She is from Brazil and had a bachelor’s and a master’s degree in statistics. She has some previous work in the Survival Analysis field in which she adapted a Current Status Data model to incorporate Misclassification. During her master’s, she worked with the Singular Decomposition Value theorem to find latent maps for cancers databases from different countries. She also has experience working as a statistician at a bank and at a marketing company. She loves to work developing models and methodologies that require a strong computational base and she loves to code using R and Python.\nAbout the Linkage Seminar Series\nThe Massive Data Institute at Georgetown University’s McCourt School of Public Policy co-hosts monthly Linkage Seminars with the American Statistical Association’s Record Linkage Interest Group. These monthly seminars feature interdisciplinary data experts discussing their work on data linkages across data types, sectors, domains, and disciplines.\n\n\n\n",
    "preview": "posts/2022-02-16-youre-invited-linkage-seminar-series-thursday-february-24/portrait.jpg",
    "last_modified": "2022-02-16T17:58:47-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-23-record-linkage-at-the-gpsg-community-pantry/",
    "title": "Record Linkage at the Duke GPSG Community Pantry",
    "description": "The Duke Graduate and Professional Student Government (GPSG) Community Pantry is a student-operated food pantry serving the student community at Duke University. In this post, I describe the record linkage system used at the Pantry to identify individual customers and obtain their order history. This is done using a Python module for deterministic record linkage and model evaluation techniques which I describe in detail.",
    "author": [
      {
        "name": "Olivier Binette",
        "url": "https://olivierbinette.github.io"
      }
    ],
    "date": "2021-12-23",
    "categories": [
      "Python",
      "Record Linkage"
    ],
    "contents": "\n\nContents\nIntroduction\nFood Insecurity on Campus\n\nThe Record Linkage Approach\nDeterministic Record Linkage Rule\nImplementation\nLimitations\n\nModel Evaluation\nPairwise Precision and Recall\nResults\n\nFinal thoughts\nAbout the Author\n\nFigure from https://gpsg.duke.edu/resources-for-students/community-pantry/Introduction\nDuke’s Graduate and Professional Student Government (GPSG) has been operating a community food pantry for about five years. The pantry provides nonperishable food and basic need items to graduate and professional students on campus. There is a weekly bag program, where students order customized bags of food to be picked up on Saturdays, as well as an in-person shopping program open on Thursdays and Saturdays.\n\nFigure 1: Weekly number of customers at the Pantry. The black line is a moving average of weekly visits.\n\nThe weekly bag program, which began in May 2018 and is still the most popular pantry offering, provides quite a bit of data regarding pantry customers and their habits. Some customers have ordered more than 80 times in the past 2 years, while others only ordered once or twice. For every order, we have the customer’s first name and last initial, an email address (which became mandatory around mid 2018), a phone number in a few cases, an address in some cases (for delivery), we have demographic information some cases, and we have the food order information. Available quasi-identifying information is shown in Table 1 below.\nTable 1: Quasi-identifying information provided on Qualtrics bag order forms. Note that phone number and address were only required while delivery was offered. Furthermore, most customers stop answering demographic questions after a few orders.\nQuestion no.\nQuestion\nAnswer form\nMandatory?\n-\nIP address\n-\nYes\n2\nFirst name and last initial\nFree form\nYes\n3\nDuke email\nFree form\nYes\n4\nPhone number\nFree form\nNo\n6\nAddress\nFree form\nNo\n8\nFood allergies\nFree form\nNo\n9\nNumber of members in household\n1-2 or 3+\nYes\n10\nWant baby bag?\nYes or no\nYes\n30\nDegree\nMultiple choices or Other\nNo\n31\nSchool\nMultiple choices or Other\nNo\n32\nYear in graduate school\nMultiple choices\nNo\n33\nNumber of adults in household\nMultiple choices\nNo\n34\nNumber of children in household\nMultiple choices\nNo\nGaining the most insight from this data requires linking order records from the same customer. Identifying individual customers and associating them with an order history allows us to investigate shopping recurrence patterns and identify potential issues with the pantry’s offering. For instance, we can know who stopped ordering from the pantry after the home delivery program ended. These are people who, most likely, do not have a car to get to the pantry but might benefit from new programs, such as a ride-share program or a gift card program.\nThis blog post describes the way in which records are linked at the Community Pantry. As we will see, the record linkage problem is not particularly difficult. It is not trivial either, however, and it does require care to ensure that it runs reliably and efficiently, and that it is intelligible and properly validated. This post goes in detail into these two aspects of the problem.\nRegarding efficiency and reliability of the software system, I describe the development of a Python module, called GroupByRule, for record linkage at the pantry. This Python module is maintainable, documented and tested, ensuring reliability of the system and the potential for its continued use throughout the years, even as technical volunteers change at the pantry. Regarding validation of the record linkage system, I describe simple steps that can be taken to evaluate model performance.\nBefore jumping into the technical part, let’s take a step back to discuss the issue of food insecurity on campus.\nFood Insecurity on Campus\nIt is often surprising to people that some Duke students might struggle having access to food. After all, Duke is one of the richest campuses in the US with its 12 billion endowment, high tuition and substantial research grants. Prior to the covid-19 pandemic, this wealth could be seen on campus and benefit many. Every weekday, there were several conferences and events with free food. Me and many other graduate students would participate in these events, earning 3-4 free lunches every week. Free food on campus is now a thing of the past, for the most part.\nHowever, free lunch or not, it’s important to realize the many financial challenges which students can face. International students on F-1 and J-1 visas have limited employment opportunities in the US. Many graduate students are married, have children or have other dependents which may not be eligible to work in the US either. Even if they are lucky enough to be paid a 9 or 12-month stipend, this stipend doesn’t go very far. For other students, going to Duke means living on a mixture of loans, financial aid, financial support from parents, and side jobs. Any imbalance in this rigid system can leave students having to compromise between their education and their health.\nA 2019 study from the World Food Policy Center reported that about 19% of graduate and professional students at Duke experienced food insecurity in the past year. This means they were unable to afford a balanced and sufficient diet, they were afraid of not having enough money for food, or they skipped meals and went hungry due to lack of money. The GPSG Community Pantry has been leading efforts to expand food insecurity monitoring on campus – we are hoping to have more data in 2022 and in following years.\nThe Record Linkage Approach\nThe bag order form contains email addresses which are highly reliable for linkage. If two records have the same email, we know for certain that they are from the same customer. However, customers do not always enter the same email address when submitting orders. Despite the request to use a Duke email address, some customers use personal emails. Furthermore, Duke email addresses have two forms. For instance, my duke email is both ob37@duke.edu and olivier.binette@duke.edu. Emails are therefore not sufficient for linkage. Phone numbers can be used as well, but these are only available for the period when home delivery was available.\nFirst name and last initial can be used to supplement emails and phone numbers. Again, agreement on first name and last initial provides strong evidence for match. On the other hand, people do not always enter their names in the same way.\nCombining the use of emails, phone numbers, and names, we may therefore link records which agree on any one of these attributes. This is a simple deterministic record linkage approach which should be reliable enough for the data analysis use of the pantry.\nDeterministic Record Linkage Rule\nTo be more precise, record linkage proceeds as follows:\nRecords are processed to clean and standardize the email, phone and name attributes. That is, leading and trailing whitespace are removed, capitalization is standardized, phone numbers are validated and standardized, and punctuation is removed from names.\nRecords which agree on any of their email, phone or name attributes are linked together.\nConnected components of the resulting graph are computed in order to obtain record clusters.\nThis record linkage procedure is extremely simple. It relies the fact that all three attributes are reliable indicators of a match and that, for two matching records, it is likely that at least one of these three attributes will be in agreement.\nAlso, the simplicity of the approach allows the use of available additional information (such as IP address and additional questions) for model validation. If the use of this additional information does not highlight any flaws with the simple deterministic approach, then this means that the deterministic approach is already good enough. We will come back to this when discussing model validation techniques.\nImplementation\nOur deterministic record linkage system is implemented in Python with some generality. The goal is for the system to be able to adapt to changes in data or processes.\nThe fundamental component of the system is a LinkageRule class. LinkageRule objects can be fitted to data, providing either a clustering or a linkage graph. For instance, a LinkageRule might be a rule to link all records which agree on the email attribute. Another LinkageRule might summarize a set of other rules, such as taking the union or intersection of their links.\nThe interface is as follows:\n\nfrom abc import ABC, abstractmethod\n\n\nclass LinkageRule(ABC):\n    \"\"\"\n    Interface for a linkage rule which can be fitted to data.\n\n    This abstract class specifies three methods. The `fit()` method fits the \n    linkage rule to a pandas DataFrame. The `graph` property can be used after \n    `fit()` to obtain a graph representing the linkage fitted to data.  The \n    `groups` property can be used after `fit()` to obtain a membership vector \n    representing the clustering fitted to data.\n    \"\"\"\n    @abstractmethod\n    def fit(self, df):\n        pass\n\n    @property\n    @abstractmethod\n    def graph(self):\n        pass\n\n    @property\n    @abstractmethod\n    def groups(self):\n        pass\n\nNote that group membership vectors, our representation for cluster groups, are meant to be a numpy integer array with entries indicating what group (cluster) a given record belongs to. Such a “groups” vector should not contain NA values; rather it should contain distinct integers for records that are not in the same cluster.\nWe will now define two other classes, Match and Any, which allow us to implement deterministic record linkage. The Match class implements an exact matching rule, while Any is the logical disjunction of a given set of rules. Our deterministic record linkage rule for the pantry will therefore be defined as follows:\n\nrule = Any(Match(\"name\"), Match(\"email\"), Match(\"phone\"))\n\nFollowing the LinkageRule interface, this rule will then be fitted to the data and used as follows:\n\nrule.fit(data)\ndata.groupby(rule.groups).last() # Get last visit data for all customers.\n\nThe benefit of this general interface is that it is extendable. By default, the Any class will return connected components when requesting group clusters. However, other clustering approaches could be used. Exact matching rules could also be relaxed to fuzzy matching rules based on string distance metrics or probabilistic record linkage. All of this can be implemented as additional LinkageRule subclasses in a way which is compatible with the above.\nLet’s now work on the Match class. For efficiency, we’ll want Match to operate at the groups level. That is, if Match is called on a set of rules, then we’ll first compute groups for these rules, before computing the intersection of these groups. This core functionality is implemented in the function _groups_from_rules() below. The function _groups() is a simple wrapper to interpret strings as a matching rule on the corresponding column.\n\nimport pandas as pd\nimport numpy as np\nimport itertools\nfrom igraph import Graph\n\ndef _groups(rule, df):\n    \"\"\"\n    Fit linkage rule to dataframe and return membership vector.\n\n    Parameters\n    ----------\n    rule: string or LinkageRule\n        Linkage rule to be fitted to the data. If `rule` is a string, then this \n        is interpreted as an exact matching rule for the corresponding column.\n    df: DataFrame\n        pandas Dataframe to which the rule is fitted.\n\n    Returns\n    -------\n    Membership vector (i.e. integer vector) u such that u[i] indicates the \n    cluster to which dataframe row i belongs. \n\n    Notes\n    -----\n    NA values are considered to be non-matching.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> df = pd.DataFrame({\"fname\":[\"Olivier\", \"Jean-Francois\", \"Alex\"], \n      \"lname\":[\"Binette\", \"Binette\", pd.NA]})\n\n    Groups specified by distinct first names:\n    >>> _groups(\"fname\", df)\n    array([2, 1, 0], dtype=int8)\n\n    Groups specified by same last names:\n    >>> _groups(\"lname\", df)\n    array([0, 0, 3], dtype=int8)\n\n    Groups specified by a given linkage rule:\n    >>> rule = Match(\"fname\")\n    >>> _groups(rule, df)\n    array([2, 1, 0])\n    \"\"\"\n    if (isinstance(rule, str)):\n        arr = np.array(pd.Categorical(df[rule]).codes, dtype=np.int32) # Specifying dtype avoids overflow issues\n        I = (arr == -1)  # NA value indicators\n        arr[I] = np.arange(len(arr), len(arr)+sum(I))\n        return arr\n    elif isinstance(rule, LinkageRule):\n        return rule.fit(df).groups\n    else:\n        raise NotImplementedError()\n\n\ndef _groups_from_rules(rules, df):\n    \"\"\"\n    Fit linkage rules to data and return groups corresponding to their logical \n    conjunction.\n\n    This function computes the logical conjunction of a set of rules, operating \n    at the groups level. That is, rules are fitted to the data, membership \n    vector are obtained, and then the groups specified by these membership \n    vectors are intersected.\n\n    Parameters\n    ----------\n    rules: list[LinkageRule]\n        List of strings or Linkage rule objects to be fitted to the data. \n        Strings are interpreted as exact matching rules on the corresponding \n        columns.\n\n    df: DataFrame\n        pandas DataFrame to which the rules are fitted.\n\n    Returns\n    -------\n    Membership vector representing the cluster to which each dataframe row \n    belongs.\n\n    Notes\n    -----\n    NA values are considered to be non-matching.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> df = pd.DataFrame({\"fname\":[\"Olivier\", \"Jean-Francois\", \"Alex\"], \n      \"lname\":[\"Binette\", \"Binette\", pd.NA]})\n    >>> _groups_from_rules([\"fname\", \"lname\"], df)\n    array([2, 1, 0])\n    \"\"\"\n\n    arr = np.array([_groups(rule, df) for rule in rules]).T\n    groups = np.unique(arr, axis=0, return_inverse=True)[1]\n    return groups\n\nWe can now implement Match as follows. Note that the Graph representation of the clustering is only computed if and when needed.\n\nclass Match(LinkageRule):\n    \"\"\"\n    Class representing an exact matching rule over a given set of columns.\n\n    Attributes\n    ----------\n    graph: igraph.Graph\n        Graph representing linkage fitted to the data. Defaults to None and is \n        instantiated after the `fit()` function is called.\n\n    groups: integer array\n        Membership vector for the linkage clusters fitted to the data. Defaults \n        to None and is instantiated after the `fit()` function is called.\n\n    Methods\n    -------\n    fit(df)\n        Fits linkage rule to the given dataframe.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> df = pd.DataFrame({\"fname\":[\"Olivier\", \"Jean-Francois\", \"Alex\"], \n    \"lname\":[\"Binette\", \"Binette\", pd.NA]})\n\n    Link records which agree on both the \"fname\" and \"lname\" fields.\n    >>> rule = Match(\"fname\", \"lname\")\n\n    Fit linkage rule to the data.\n    >>> _ = rule.fit(df)\n\n    Construct deduplicated dataframe, retaining only the first record in each cluster.\n    >>> _ = df.groupby(rule.groups).first()\n    \"\"\"\n\n    def __init__(self, *args):\n        \"\"\"\n        Parameters\n        ----------\n        args: list containing strings and/or LinkageRule objects.\n            The `Match` object represents the logical conjunction of the set of \n            rules given in the `args` parameter. \n        \"\"\"\n        self.rules = args\n        self._update_graph = False\n        self.n = None\n\n    def fit(self, df):\n        self._groups = _groups_from_rules(self.rules, df)\n        self._update_graph = True\n        self.n = df.shape[0]\n\n        return self\n\n    @property\n    def groups(self):\n        return self._groups\n\nOne more method is needed to complete the implementation of a LinkageRule, namely the graph property. This property returns a Graph object corresponding to the matching rule. The graph is built as follows. First, we construct an inverted index for the clustering. That is, we construct a dictionary associating to each cluster the nodes which it contains. Then, an edge list is obtained by linking all pairs of nodes which belong to the same cluster. Note that the pure Python implementation below if not efficient for large clusters. This is not a problem for now since we will generally avoid computing this graph.\n\n# Part of the definition of the `Match` class:\n    @property\n    def graph(self) -> Graph:\n        if self._update_graph:\n            # Inverted index\n            clust = pd.DataFrame({\"groups\": self.groups}\n                                 ).groupby(\"groups\").indices\n            self._graph = Graph(n=self.n)\n            self._graph.add_edges(itertools.chain.from_iterable(\n                itertools.combinations(c, 2) for c in clust.values()))\n            self._update_graph = False\n        return self._graph\n\nFinally, let’s implement the Any class. It’s purpose is to take the union (i.e. logical disjunction) of a set of rules. Just like for Match, we can choose to operate at the groups or graph level. Here we’ll work at the groups level for efficiency. That is, given a set of rules, Any will first compute their corresponding clusters before merging overlapping clusters.\nThere are quite a few different ways to efficiently merge clusters. Here we’ll merge clusters by computing a “path graph” representation, taking the union of these graphs, and then computing connected components. For a given clustering, say containing records a, b, and c, the “path graph” links records as a path a–b–c.\nFirst, we define the functions needed to compute path graphs:\n\ndef pairwise(iterable):\n    \"\"\"\n    Iterate over consecutive pairs:\n        s -> (s[0], s[1]), (s[1], s[2]), (s[2], s[3]), ...\n\n    Note\n    ----\n    Current implementation is from itertools' recipes list available at \n    https://docs.python.org/3/library/itertools.html\n    \"\"\"\n    a, b = itertools.tee(iterable)\n    next(b, None)\n    return zip(a, b)\n\n\ndef _path_graph(rule, df):\n    \"\"\"\n    Compute path graph corresponding to the rule's clustering: cluster elements \n    are connected as a path.\n\n    Parameters\n    ----------\n    rule: string or LinkageRule\n        Linkage rule for which to compute the corresponding path graph \n        (strings are interpreted as exact matching rules for the corresponding column).\n    df: DataFrame\n        Data to which the linkage rule is fitted.\n\n    Returns\n    -------\n    Graph object such that nodes in the same cluster (according to the fitted \n    linkage rule) are connected as graph paths.\n    \"\"\"\n    gr = _groups(rule, df)\n    \n    # Inverted index\n    clust = pd.DataFrame({\"groups\": gr}\n                         ).groupby(\"groups\").indices\n    graph = Graph(n=df.shape[0])\n    graph.add_edges(itertools.chain.from_iterable(\n        pairwise(c) for c in clust.values()))\n\n    return graph\n\nWe can now implement the Any class:\n\nclass Any(LinkageRule):\n    \"\"\"\n    Class representing the logical disjunction of linkage rules.\n\n    Attributes\n    ----------\n    graph: igraph.Graph\n        Graph representing linkage fitted to the data. Defaults to None and is \n        instantiated after the `fit()` function is called.\n\n    groups: integer array\n        Membership vector for the linkage clusters fitted to the data. Defaults \n        to None and is instantiated after the `fit()` function is called.\n\n    Methods\n    -------\n    fit(df)\n        Fits linkage rule to the given dataframe.\n    \"\"\"\n\n    def __init__(self, *args):\n        \"\"\"\n        Parameters\n        ----------\n        args: list containing strings and/or LinkageRule objects.\n            The `Any` object represents the logical disjunction of the set of \n            rules given by `args`. \n        \"\"\"\n        self.rules = args\n        self._graph = None\n        self._groups = None\n        self._update_groups = False\n\n    def fit(self, df):\n        self._update_groups = True\n        graphs_vect = [_path_graph(rule, df) for rule in self.rules]\n        self._graph = igraph.union(graphs_vect)\n        return self\n\n    @property\n    def groups(self):\n        if self._update_groups:\n            self._update_groups = False\n            self._groups = np.array(\n                self._graph.clusters().membership)\n        return self._groups\n\n    @property\n    def graph(self) -> Graph:\n        return self._graph\n\nThe complete Python module (still under development) implementing this approach can be found on Github at OlivierBinette/GroupByRule.\nLimitations\nThere are quite a few limitations with this simple deterministic approach. We’ll see in the model evaluation section that these do not affect performance to a large degree. However, for a system used with more data or over a longer timeframe, these should be carefully considered.\nFirst, the deterministic linkage does not allow the consideration of contradictory evidence. For instance, if long-form Duke email addresses are provided on two records and do not agree (e.g. “olivier.binette@duke.edu” and “olivier.bonhomme@duke.edu” are provided), then we know for sure that the records do not correspond to the same individual, even if the same name was provided (here Olivier B.). The consideration of such evidence could rely on probabilistic record linkage, where each record pair is associated a match probability.\nSecond, the use of connected components to resolve transitivity can be problematic, as a single spurious link could connect two large clusters by mistake. More sophisticated graph clustering techniques, in combination with probabilistic record linkage, would be required to mitigate the issue.\nModel Evaluation\nI cannot share any of the data which we have at the Pantry. However, I can describe general steps to be taken to evaluate model performance in practice.\nPairwise Precision and Recall\nHere we will evaluate linkage performance using pairwise precision \\(P\\) and recall \\(R\\). The precision \\(P\\) is defined as the proportion of predicted links which are true matches, whereas \\(R\\) is the proportion of true matches which are correctly predicted. That is, if \\(TP\\) is the number of true positive links, \\(P\\) the number of predicted links, and \\(T\\) the number of true matches, then we have \\[\nP = TP/P, \\quad R = TP/T.\n\\]\nEstimating Precision\nIt is helpful to express precision and recall in cluster form, where cluster elements are all interlinked. Let \\(C\\) be the set of true clusters and let \\(\\hat C\\) be the set of predicted clusters. For a given cluster \\(\\hat c \\in \\hat C\\), let \\(C \\cap \\hat c\\) be the restriction of the clustering \\(C\\) to \\(\\hat c\\). Then we have \\[\n  P = \\frac{\\sum_{\\hat c \\in \\hat C} \\sum_{e \\in C \\cap \\hat c} {\\lvert e\\rvert \\choose 2} }{ \\sum_{\\hat c \\in \\hat C} {\\lvert \\hat c \\rvert \\choose 2}}.\n\\]\nThe denominator can be computed exactly, while the numerator can be estimated by randomly sampling clusters \\(\\hat c \\in \\hat C\\), breaking them up into true clusters \\(e \\in C \\cap \\hat c\\), and then computing the sum of the combinations \\({\\lvert e\\rvert \\choose 2}\\). Importance sampling could be used to reduce the variance of the estimator, but it does not seem necessary for the scale of the data which we have at the pantry, where each predicted cluster can be examined quite quickly.\nIn practice, the precision estimation process can be carried out as follows:\nSample predicted clusters at random (in the case of the pantry, we can take all predicted clusters).\nMake a spreadsheet with all the records corresponding to the sampled clusters.\nSort the spreadsheet by predicted cluster ID.\nAdd a new empty column to the spreadsheet, called “trueSubClusters.”\nSeparately look at each predicted cluster. If the cluster should be broken up in multiple parts, use the “trueSubClusters” column to provide identifiers for true cluster membership. Note that these identifiers do not need to match across predicted clusters.\nThe spreadsheet can then be read-in and processed in a straightforward way to obtain an estimated precision value.\nEstimating Recall\nEstimating recall is a bit trickier than estimating precision, but we can make one assumption to simplify the process. Assume that precision is exactly 1, or very close to 1, so that all predicted clusters can roughly be taken at face value. Estimating recall then boils to the problem of identifying which predicted clusters should be merged together.\nIndeed, using the same notations as above, we can write \\[\nR = \\frac{\\sum_{ c \\in  C} \\sum_{e \\in \\hat C \\cap  c} {\\lvert e\\rvert \\choose 2} }{ \\sum_{ c \\in  C} {\\lvert  c \\rvert \\choose 2}}.\n\\] If precision is 1, then the denominator can be computed from the sizes of merged predicted clusters. On the other hand, the nominator simplifies to \\(\\sum_{e \\in \\hat C}{\\lvert e \\rvert \\choose 2}\\) which can be computed exactly from the sizes of predicted clusters. In the case of the Pantry, wrongly separated clusters are likely to be due to small differences in names and emails. Therefore, we can identify clusters which should have been merged together as follows:\nMake a spreadsheet containing canonical customer records (one representative record for each predicted individual customer).\nCreate a new empty column named “trueClustersA.”\nSort the spreadsheet by name.\nGo through the spreadsheet from top to bottom, looking at whether or not consecutive predicted clusters should be merged together. If so, write a corresponding cluster membership ID in the “trueClustersA” column.\nCreate a new empty column named “trueClustersB.”\nSort the spreadsheet by email\nGo through the spreadsheet from top to bottom, looking at whether or not consecutive predicted clusters should be merged together. If so, write a corresponding cluster membership ID in the “trueClustersB” column.\nThis process might not catch all wrongly separated clusters, but it is likely to find many of the errors due to different ways of writing names and different email addresses. The resulting spreadsheet can then easily be processed to obtain an estimated recall. If we were working with a larger dataset, we’d have to use further blocking to restrict our consideration to a more manageable subset of the data.\nResults\nI used the above procedures to estimate precision and recall of our simple deterministic approach to deduplicate the Pantry’s data. There was a total of 3281 bag order records for 689 estimated customers. The results are below.\nEstimated Precision: 92%\nPrecision is somewhat low due to about 3 relatively large clusters (around 30-50 records each) which should have been broken up in a few parts. 2% precision was lost due to a couple that shared a phone number, where each had about 20 order records. The vast majority of spurious links were tied to bag orders for which only the first name was provided (e.g. “Sam”). The use of negative evidence to distinguish between individuals would help resolve these cases.\nEstimated Recall: 99.6%\nThis is certainly an underestimate, but it does show that missing links are not obviously showing up. Given the structure of the Pantry data, it is likely that recall is indeed quite high.\nFinal thoughts\nThere are many ways in which the record linkage approach could be improved. As previously discussed, probabilistic record linkage would allow the consideration of negative evidence and the use of additional quasi-identifying information (such as IP addresses and other responses on the bag order forms). I’m looking forward to building on the GroupByRule Python module to provide a user-friendly and unified interface to more flexible methodology.\nHowever, it is important to ensure that any record linkage approach is intelligible and rooted in a good understanding of the underlying data. In this context, the use of a well-thought deterministic approach can provide good performance, at least as a first step or baseline for comparison. Furthermore, it is important to spend sufficient time investigating the results of the linkage to evaluate performance. I have highlighted simple steps which can be taken to estimate precision and make a good effort at identifying missing links. This is highly informative for model validation, improvement, and for the interpretation of any following results.\nAbout the Author\nOlivier Binette is a PhD Candidate in the Statistical Science Department at Duke University. He works part-time as research coordinator at the GPSG Community Pantry.\n\n\n\nCampbell, Kevin M., Dennis Deck, and Antoinette Krupski. 2008. “Record Linkage Software in the Public Domain: A Comparison of Link Plus, the Link King, and a ’Basic’ Deterministic Algorithm.” Health Informatics Journal 14 (1): 5–15.\n\n\nGomatam, Shanti, Randy Carter, Mario Ariet, and Glenn Mitchell. 2002. “An Empirical Comparison of Record Linkage Procedures.” Statistics in Medicine 21 (10): 1485–96. https://doi.org/10.1002/sim.1147.\n\n\nMonge, Alvaro E., and Charles P. Elkan. 1997. “An Efficient Domain-Independent Algorithm for Detecting Approximately Duplicate Database Records.” Proceedings of the SIGMOD 1997 Workshop on Research Issues on Sata Mining and Knowledge Discovery, 23–29. https://doi.org/10.1.1.28.8405.\n\n\nPotosky, Arnold L., Gerald F. Riley, James D. Lubitz, Renee M. Mentnech, and Larry G. Kessler. 1993. “Potential for Cancer Related Health Services Research Using a Linked Medicare-Tumor Registry Database.” Medical Care 31 (8): 732–48. https://doi.org/10.1097/00005650-199308000-00006.\n\n\nTromp, Miranda, Anita C. Ravelli, Gouke J. Bonsel, Arie Hasman, and Johannes B. Reitsma. 2011. “Results from Simulated Data Sets: Probabilistic Record Linkage Outperforms Deterministic Record Linkage.” Journal of Clinical Epidemiology 64 (5): 565–72. https://doi.org/10.1016/j.jclinepi.2010.05.008.\n\n\n\n\n",
    "preview": "http://dukegpsc.org/wp-content/uploads/2019/07/pantry.jpg",
    "last_modified": "2022-02-17T15:13:10-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-20-welcome/",
    "title": "Welcome!",
    "description": "Welcome to the blog of ASA's Record Linkage Interest Group.",
    "author": [
      {
        "name": "Olivier Binette",
        "url": "https://olivierbinette.github.io"
      }
    ],
    "date": "2021-12-20",
    "categories": [],
    "contents": "\n\nContents\nHow to Contribute\nCode of Conduct\n\nWelcome! This is the blog of the American Statistical Association’s Record Linkage Interest Group (RLIG).\nIf you are interested in deterministic and probabilistic record linkage, entity resolution, data fusion, or statistical matching, then this is for you. We hope you become a member of RLIG (it’s free) to stay up to date with news and events in our community. You can learn more on our main website at https://sites.google.com/view/rlig/.\nWe hope this blog will help foster community within RLIG. We encourage everyone to contribute. Posts can be short, such as to share interesting papers and associated thoughts, or they can be longer to go into any topic you find interesting. You can comment on posts to get in touch with their authors.\nFirst and foremost, we want this blog to be a welcoming place for all. There is no “right” way to write posts – express yourself as you like. We want to hear your unique perspective. Reply to comments, welcome feedback, and make connections!\nInstructions to write blog posts are available on the Contribute page. It might seem a bit overwhelming if you have not worked with these tools before. However, we are here to help. If you’d like to post something but don’t want to use R, we’ll create the post for you. If you’re unsure about any of the steps below, get in touch and we’ll provide support. Contact me, Olivier Binette, for any question. I’ll be more than happy to help you contribute to the blog.\nHow to Contribute\nSee the Contribute page for information on how to contribute to this blog.\nCode of Conduct\nPlease note that the RecordLinkageIG.github.io project is released with a Contributor Code of Conduct. By contributing to this project, you agree to abide by its terms.\n\n\n\n",
    "preview": "posts/2021-12-20-welcome/wave.png",
    "last_modified": "2022-02-14T15:24:48-05:00",
    "input_file": {},
    "preview_width": 512,
    "preview_height": 512
  }
]
