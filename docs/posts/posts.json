[
  {
    "path": "posts/2022-09-06-call-for-speakers/",
    "title": "JSM 2023: Call for Speakers for Record Linkage Session",
    "description": "We are looking for speakers!",
    "author": [
      {
        "name": "RLIG",
        "url": {}
      }
    ],
    "date": "2022-09-06",
    "categories": [
      "Events"
    ],
    "contents": "\nThe Record Linkage Interest Group of the American Statistical\nAssociation would like to submit an Invited Session proposal for JSM\n2023. The Government Statistics section will co-sponsor.\nWe are looking for speakers!\nAll we need for the application is a name and affiliation. The exact\nnature of your talk is not needed right now. Are you thinking about\nattending JSM 2023 in Toronto and would you be interested in giving a\ntalk on the theme of ‚ÄúRecord Linkage in Government Statistics‚Äù? Thursday\nis the due date for the proposal, so please respond to mlarsen\nat smcvt.edu ASAP if you are interested.\nSession Information\nTitle: Record Linkage in Government Statistics\nKeywords: Data fusion; De-duplication; Entity\nresolution; Fellegi-Sunter theory; File matching\nAbstract: Record linkage (RL) is the process of\nidentifying entries in two or more databases that refer to common\nindividuals, businesses, or, more generally, entities. RL sometimes is\nreferred to as data fusion, de-duplication, or entity resolution. In\ngovernment applications, the databases used in RL can include censuses,\nsurveys, and administrative data sources. RL can be used to remove\nduplicate entries, enhance information on entities by merging variables\nfrom multiple sources, augment survey frames and auxiliary variables,\nreduce respondent burden, and improve data quality. RL is used by many\nofficial statistical agencies around the world. RL is challenging when\nfiles are very large, information for linkage is limited, errors are\npresent in some variables, a population structure is complex, and\nentities can have multiple varying representations over time. High\nquality record linkage requires expertise in statistics and computing\nand careful attention to details of variables and files. This session\npresents developments in government record linkage applications,\nmethods, and theory.\n\n\n\n",
    "preview": "posts/2022-09-06-call-for-speakers/jsm2023.png",
    "last_modified": "2022-09-06T20:35:47-04:00",
    "input_file": {},
    "preview_width": 400,
    "preview_height": 251
  },
  {
    "path": "posts/2022-08-03-rlig-breakfast-at-jsm/",
    "title": "RLIG Breakfast at JSM on Thursday August 11!",
    "description": "Come join RLIG at the Joint Statistical Meetings (JSM) conference on Thursday, August 11, from 7:00 - 8:00 AM in room CC-210",
    "author": [
      {
        "name": "RLIG",
        "url": {}
      }
    ],
    "date": "2022-08-03",
    "categories": [
      "Events"
    ],
    "contents": "\nInformation about the\nsocial breakfast:\nMeeting Title: Record Linkage Interest Group\nDate: Thursday, 8/11/2022\nStart Time: 7:00 AM\nEnd Time: 8:00 AM\nMeeting Room: CC- 210\nRoom Set: Rounds\n\n\n\n",
    "preview": {},
    "last_modified": "2022-09-06T20:16:15-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-04-19-linkage-seminar-series-thursday-april-21/",
    "title": "üì¢ Linkage Seminar Series - Thursday April 21",
    "description": "David Beauchemin will discuss how duplicate detection can be used with AI and NLP techniques to extract information from external data sources using text distance metrics (e.g. Jaro) and classification algorithms. He will use a case study in insurance to demonstrate the proposed approach.",
    "author": [
      {
        "name": "RLIG",
        "url": {}
      }
    ],
    "date": "2022-04-19",
    "categories": [
      "Events"
    ],
    "contents": "\nYou are invited\n\n\n\nMDI and the ASA Record Linkage Interest Group‚Äôs Linkage Seminar\nSeries: \nAI and duplicate detection to leverage external data\nsources \nThursday, April 21, 2022\n1 PM ET\n Register\nHere \nZoom Webinar\n\nDavid Beauchemin will discuss how duplicate\ndetection can be used with AI and NLP techniques to extract information\nfrom external data sources using text distance metrics (e.g.¬†Jaro) and\nclassification algorithms. He will use a case study in insurance to\ndemonstrate the proposed approach.\nThis seminar will be recorded and resources will be posted\nonline after the event.\nGuest Speaker: David\nBeauchemin\n\n\nDavid Beauchemin\nAn actuary by training, David decided to continue his studies at the\nmaster‚Äôs level in computer science to become familiar with machine\nlearning. His master‚Äôs degree in natural language processing (NLP)\nfocused on external source extraction in an insurance business process.\nHe is now pursuing his studies at the Ph.D.¬†level, where he is\ninterested in personalizing automatically generated content from\ninsurance contracts.\nAbout the Linkage Seminar\nSeries\nThe Massive Data Institute at Georgetown University‚Äôs McCourt School\nof Public Policy co-hosts monthly Linkage Seminars with the American\nStatistical Association‚Äôs Record Linkage Interest Group. These monthly\nseminars feature interdisciplinary data experts discussing their work on\ndata linkages across data types, sectors, domains, and disciplines.\n\n\n\n",
    "preview": "posts/2022-04-19-linkage-seminar-series-thursday-april-21/unnamed.jpg",
    "last_modified": "2022-09-06T20:16:15-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-29-record-linkage-methodology-under-fellegi-sunter-paradigm-with-extensions/",
    "title": "üì¢ Record Linkage Methodology Under Fellegi-Sunter Paradigm, with Extensions - Thursday April 7",
    "description": "In this talk, Mr. Resnick will give an overview of the Fellegi-Sunter approach, explaining how candidate pair are evaluated under it. He will also cover extensions and modifications to it.",
    "author": [
      {
        "name": "RLIG",
        "url": {}
      }
    ],
    "date": "2022-03-29",
    "categories": [
      "Events"
    ],
    "contents": "\nYou are invited\n\nRecord Linkage Methodology Under Fellegi-Sunter\nParadigm, with Extensions \nThursday, April 7, 2022\n12 PM PT\n Register\nHere \nZoom Webinar\n\nIn 1969, Ivan Fellegi and Alan Sunter formalized a strategy for\nconducting probabilistic record linkage that had been developed\npreviously. Included in this formalization was the demonstration that\nthe scoring method used with this is optimal under certain assumptions.\nWhile other record linkage methods have been developed (including\nBayesian-based ones) for large-scale linkages the Fellegi-Sunter\napproach should be a strong candidate\nIn this talk, Mr.¬†Resnick will give an overview of the Fellegi-Sunter\napproach, explaining how candidate pair are evaluated under it. He will\nalso cover extensions and modifications to it, which include the\nfollowing:\nData editing and other preparation\nEstimation of scoring parameters using machine learning (E-M\nalgorithm)\nUse of name (and other comparison variable) frequencies\nUse of partial string agreements\nHierarchical (nested) comparisons\nUse of blocking and development of optimal blocking strategies\nEstimation of match probability and linkage error\nThis webinar is part of the Advanced\nMethods Webinar Series.\nSpeaker: Dean Resnick\nDean Resnick\nMr Resnick is a principal data scientist with NORC at the University\nof Chicago. He has been working on data analysis and statistical\nprogramming for at least several decades. Most of this work has focused\non using survey and administrative data for policy analysis, often in\nthe healthcare domain. For more than 10 years he worked at the U.S.\nCensus Bureau in the administrative record area. Here he became familiar\nwith record linkage, where it was being used to link very large surveys,\nenumerations, and administrative record files. During this period, he\ndeveloped a SAS based record-linkage module for high-volume linkages\nthat is still being used at the Bureau. At NORC, much of his work is\nfocused on record linkage and he has developed (in collaboration with\ncolleagues) a new SAS-based record linkage package that incorporates the\nE-M algorithm and several enhanced strategies for improving the quality\nof record linkage analyses.\n\n\n\n",
    "preview": "https://www.popdata.bc.ca/sites/default/files/inline-images/logos/PDBC_pantones_nowords.jpg",
    "last_modified": "2022-09-06T20:16:15-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-25-linkage-seminar-series-friday-april-1st/",
    "title": "üì¢ Linkage Seminar Series - Friday April 1st",
    "description": "Roee Gutman will present work that view record linkage as a missing data problem and he will describe Bayesian procedures that utilize data features that are frequently encountered in public health applications.",
    "author": [
      {
        "name": "RLIG",
        "url": {}
      }
    ],
    "date": "2022-03-25",
    "categories": [
      "Events"
    ],
    "contents": "\nYou are invited\n\n\n\nMDI and the ASA Record Linkage Interest Group‚Äôs Linkage Seminar\nSeries: \nBayesian Procedures for File Linking with Application to\nHealth Services Research \nFriday, April 1, 2022\n2 PM ET\n Register\nHere \nZoom Webinar\n\nRoee Gutman will present work that view record linkage as a missing\ndata problem and he will describe Bayesian procedures that utilize data\nfeatures that are frequently encountered in public health applications.\nThese procedures improve the linkage, and result in more accurate and\nprecise estimates of scientifically important associations. The first\nprocedure incorporates associations between variables exclusive to one\nof the datasets in the linkage process. The second procedure ensures\nthat individuals receiving care from the same provider in one file are\nlinked to individuals receiving care from a similar provider in the\nother file. Roee will demonstrate these procedures using two\napplications: one combines Medicare claims records and Vital Statistics\nMortality records to study the association between end-of-life medical\nexpenses and causes of death. A second application combines records from\nthe National Trauma Databank with Medicare claims data to study the\nrelationship between injury characteristics and successful discharge to\nthe community among patients with traumatic brain injury.\nThis seminar will be recorded and resources will be posted\nonline after the event.\nGuest Speaker: Roee\nGutman\n\n\nRoee Gutman\nRoee Gutman is an Associate Professor in the Department of\nBiostatistics at Brown University. His areas of expertise are causal\ninference, file linkage, missing data, Bayesian analysis and their\napplication to health services research. He has been the lead\nstatistician on multiple NIH and VA grants, and he has received two\nPCORI methods award. Roee has developed multiple procedures to link and\nanalyze healthcare data.\nAbout the Linkage Seminar\nSeries\nAbout the Linkage Seminar Series: The Massive Data Institute at\nGeorgetown University‚Äôs McCourt School of Public Policy co-hosts monthly\nLinkage Seminars with the American Statistical Association‚Äôs Record\nLinkage Interest Group. These monthly seminars feature interdisciplinary\ndata experts discussing their work on data linkages across data types,\nsectors, domains, and disciplines.\n\n\n\n",
    "preview": "posts/2022-03-25-linkage-seminar-series-friday-april-1st/portrait.jpg",
    "last_modified": "2022-09-06T20:16:15-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-03-22-bls-statistical-seminar-on-monday-march-28th/",
    "title": "üì¢ BLS Statistical Seminar on Monday, March 28th",
    "description": "Data Analysis after Record Linkage: sources of error, consequences, and possible solutions, Dr. Martin Slawski, Department of Statistics, Volgenau School of Engineering, George Mason University",
    "author": [
      {
        "name": "RLIG",
        "url": {}
      }
    ],
    "date": "2022-03-22",
    "categories": [
      "Events"
    ],
    "contents": "\nYou are invited to the next virtual BLS Statistical Seminar on\nMonday, March 28th at 2PM via the Teams link below.\nTitle\nData Analysis after Record Linkage: sources of error,\nconsequences, and possible solutions, Dr.¬†Martin Slawski, Department of\nStatistics, Volgenau School of Engineering, George Mason\nUniversity\nDate\nMonday, March 28th, 2022\nTime\n2:00PM - 3:30PM EDT\nLocation\nMicrosoft Teams (click\nhere to join the meeting)\nSpeaker\nDr.¬†Martin Slawski, Department of Statistics, Volgenau School of\nEngineering, George Mason University\nAbstract\nRecord linkage bears a lot of opportunities for creating richer data\nproducts, saving costs in data collection, reducing respondent burden,\nand avoiding response bias or measurement error. At the same time, the\npossibility of linkage error is often unaccounted for. Linkage error\narises from uncertainty about which pairs of records residing in two\nseparate files belong to the same statistical unit, and can result into\nmismatches (false positive matches) and missed matches (false negative\nmatches).\nIn this talk, we focus on mismatch error and to what extent such\nerror may contaminate downstream data analysis (e.g., regression or\nprincipal component analysis), which in turn leads to invalid\ninferences. We then provide an overview of possible statistical methods\nthat can be applied to mitigate the impact of such errors. We argue that\nthere is no universal strategy for this task; instead, the mitigation\nmethod of choice depends on several factors such as the mismatch rate,\ngoodness of fit of the model used by the data analyst, available\nknowledge about the linkage process, computational resources, and the\nanalysis question (inferential goal, statistical model) of interest.\n\n\n\n",
    "preview": "https://img.vendingmarketwatch.com/files/base/cygnus/vmw/image/2020/09/16x9/US_Bureaw_Labor_Stats_WIKIPEDIA.5f57a9653b517.png",
    "last_modified": "2022-09-06T20:16:15-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-17-deepparse/",
    "title": "deepparse",
    "description": "Deepparse: a state-of-the-art Python library for parsing multinational street addresses using deep learning.",
    "author": [
      {
        "name": "Marouane Yassine",
        "url": "https://www.linkedin.com/in/marouaneyassine1"
      },
      {
        "name": "David Beauchemin",
        "url": "https://www.linkedin.com/in/david-beauchemin/"
      }
    ],
    "date": "2022-02-17",
    "categories": [
      "Full Post"
    ],
    "contents": "\nDeepparse\nRecord linkage consists of identifying multiple entries which refer\nto the same entity within a database or other data sources. Since many\nreal-world entities are at least partially identified by their physical\nlocation, data parsing can prove itself to be quite useful in the\ncontext of record linkage.\nDeepparse (deepparse.org) is an\nopen-source python package that features state-of-the-art natural\nlanguage processing models trained to achieve the task of address\nparsing. Contrary to many existing solutions, deepparse has been created\nwith the objective of efficient multinational address parsing.\nTherefore, our models have been trained on data from 20 countries with\ndifferent languages and address formats, and yielded accuracies around\n99% when tested. In addition, we have conducted tests to evaluate the\ndegree to which these models can generalize their performance beyond the\ncountries in which they were trained. The results and details can be\nfound in our published paper Leveraging subword\nembeddings for multinational address parsing.\nIn this post, we are going to cover the four main features of\ndeepparse, namely:\nusing our pre-trained models to parse multinational addresses,\nretraining our models to improve performances on specific countries\nor address patterns,\nretraining our models using new address components,\naltering the configuration of our models to better fit your use\ncase.\nOut-of-the-box parsing\nWhether the addresses you wish to parse originate from the countries\non which deepparse‚Äôs models were trained, or whether you wish to\nexperiment with the package‚Äôs API, you can easily get started with a few\nlines of code.\nFirst, you‚Äôll need to choose one of the featured pre-trained models\nand instantiate an AddressParser.\nWe offer the three following models:\nbpemb: named after the multilingual subword\nembeddings used to train it, this model yields the best overall\nperformance but suffers from a heavy memory footprint,\nfasttext: this model‚Äôs architecture is similar to\nbpemb‚Äôs but uses FastText‚Äôs pre-trained french embeddings (http://fasttext.cc/). This\nmodel is lighter and faster than the previous one whilst still offering\na good performance,\nfasttext-light: this model is identical to the\nfasttext model, but uses an optimisation technique to make its memory\nusage even lower.\nIn addition to these models, we offer the possibility of adding an\nattention mechanism to further enhance performance.\n\nfrom deepparse.parser import AddressParser\n\nparser = AddressParser(model_type=\"bpemb\", attention_mechanism=True, device=0)\n\nIt is also possible to use a GPU to make the models‚Äô predictions\nfaster with the device argument.\nOnce the AddressParser is defined, you can parse an\naddress or a list of addresses in any language with a simple call to the\nparser.\n\naddreses = [\"ÏÑúÏö∏ÌäπÎ≥ÑÏãú Ï¢ÖÎ°úÍµ¨ ÏÇ¨ÏßÅÎ°ú3Í∏∏ 23\", \"777 Brockton Avenue, Abington MA 2351\"]\n\nparsed_addresses = parser(addreses)\n\nfor parsed_address in parsed_addresses:\n  print(parsed_address.address_parsed_components)\n\nOUT:\n\n[('ÏÑúÏö∏ÌäπÎ≥ÑÏãú', 'Province'), ('Ï¢ÖÎ°úÍµ¨', 'Municipality'), ('ÏÇ¨ÏßÅÎ°ú3Í∏∏', 'StreetName'), ('23', 'StreetNumber')]\n[('777', 'StreetNumber'), ('Brockton', 'StreetName'), ('Avenue,', 'StreetName'), ('Abington', 'Municipality'), ('MA', 'Province'), ('2351', 'PostalCode')]\n\nFine-tuning models\nAs mentioned before, deepparse‚Äôs models have been trained on a select\nnumber of countries, which means that the addresses you wish to parse\nmay not have been encountered before during training. This may lead to a\nlower parsing performance. However, you need not worry as long as you\nhave access to some labelled data for your use case (you can also check\nout our complete\ndataset. This is due to the retraining feature that enables\nfine-tuning of the pre-defined and pre-trained models in order to boost\nperformance for specific use cases.\nRetraining a model is as simple as making a call to an AddressParser‚Äôs\nretrain() method. The retrained model will be of the\nsame type as the one with which the AddressParser was\ninitialized.\n\ntraining_container = PickleDatasetContainer(‚ÄúPATH TO TRAINING DATA‚Äù)\n\naddress_parser = AddressParser(model_type=\"fasttext‚Äù)\n\naddress_parser.retrain(training_container, train_ratio=0.8, epochs=100)\n\nMultiple arguments enable you to configure the training process‚Äôs\nhyperparameters, such as the number of training epochs and the batch\nsize.\nFurthermore, if you wish to test your retrained model‚Äôs performance,\nyou can use the test() function to compute and return the\nmain accuracy on a test sample.\n\ntesting_container = PickleDatasetContainer(‚ÄúPATH TO TESTING DATA‚Äù)\n\naddress_parser.test(testing_container)\n\nIf you are wondering what the data format should be, you can look at\nthe original training data, which is openly available.\nDefining new address\ncomponents\nJust like the original models are not enough to cover all use cases,\none may also need to update the original parsing labels to better fit\ntheir needs. It‚Äôs possible (and easy) to do so during the retraining\nprocess by specifying a value for the prediction_tags\nargument of the retrain() function. The tags must be\ndefined in a dictionary of which the keys are the new tags, and the\nvalues are their respective indices starting at 0.\nFor instance, let‚Äôs suppose we wish to retrain a model to recognize\npostal boxes, towns and countries. First of all, we would need to define\na dictionary with the appropriate tags.\n\ntags = {\"po_box\": 0, \"town\": 1, \"country\": 2, \"EOS\": 3}\n\nNotice the presence of an extra tag (i.e.¬†EOS). This tag must be\npresent in the dictionary for the retraining to function correctly.\nOnce the tags are defined, we simply need to run the retraining\nprocess.\n\naddress_parser.retrain(training_container,\n                       train_ratio=0.8,\n                       epochs=100,\n                       batch_size=8,\n                       num_workers=2,\n                       prediction_tags=tag_dictionary,\n                       logging_path=logging_path)\n\nModifying models‚Äô\narchitecture\nFinally, if you are a machine/deep learning practitioner, you might\nbe interested in altering our models‚Äô architecture to experiment with\ndifferent hyperparameters. All the parsing models are\nsequence-to-sequence artificial neural networks consisting of an encoder\nand a decoder, built using LSTMs. While retraining a model, you can\neasily modify the number of layers in each part of the network and the\ndimensions of the hidden state using the seq2seq_params\nargument. Like the tags, new parameters must be defined inside a\ndictionary.\n\nseq2seq_params = {\n    \"encoder_hidden_size\": 512,\n    \"decoder_hidden_size\": 512\n}\n\naddress_parser.retrain(training_container, \n                       train_ratio=0.8,\n                       epochs=100,\n                       seq2seq_params= seq2seq_params)\n\nConclusion\nWhen address parsing stands in the way of a successful record\nlinkage, deepparse can alleviate some of the task‚Äôs complexity by\nproviding a good parsing performance which can be further enhanced using\nits retraining features.\nWe welcome contributions to the library, as well as questions. So do\nnot hesitate to stop by the Github repository\nif you have any inquiries!\nAbout the Authors\nMarouane Yassine is a data scientist at Laval\nUniversity‚Äôs Institute Intelligence and Data. With a background in\nsoftware engineering, he is passionate about deep learning and natural\nlanguage processing and loves to perform research and build solutions\nrelated to those exciting fields.\nDavid Beauchemin trained as an actuary, computer\nscientist, software engineer and holds an MSc in machine learning. He is\ncurrently a Ph.D.¬†student in machine learning. His expertise is at the\ncrossroads of insurance, laws, machine learning, software engineering,\nand the operationalization of AI systems.\n\n\n\n",
    "preview": "https://raw.githubusercontent.com/GRAAL-Research/deepparse/master/docs/source/_static/logos/deepparse.png",
    "last_modified": "2022-09-06T20:16:15-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-02-16-youre-invited-linkage-seminar-series-thursday-february-24/",
    "title": "üì¢ Linkage Seminar Series - Thursday February 24",
    "description": "For February‚Äôs Linkage Seminar, Amy O‚ÄôHara will be joined by Thais Menezes, a PhD student at SFI Centre for Research Training in Foundations of Data Science, University College Dublin. Thais‚Äô work incorporates the household structure in recording linkage matching in the Ireland census database to make the process of matching individuals easier and more accurate.",
    "author": [
      {
        "name": "RLIG",
        "url": {}
      }
    ],
    "date": "2022-02-16",
    "categories": [
      "Events"
    ],
    "contents": "\nYou are invited\n\n\n\nMDI and the ASA Record Linkage Interest Group‚Äôs Linkage Seminar\nSeries: \nHausdorff Distance: A Powerful Tool to Match Households\nin Record Linkage\nThursday, February 24, 2022\n1 - 2 PM ET\n Register\nHere \nZoom Webinar\n\nFor February‚Äôs Linkage Seminar, Amy O‚ÄôHara will be joined by Thais\nMenezes, a PhD student at SFI Centre for Research Training in\nFoundations of Data Science, University College Dublin. Thais‚Äô work\nincorporates the household structure in recording linkage matching in\nthe Ireland census database to make the process of matching individuals\neasier and more accurate.\nThis seminar will be recorded and resources will be posted\nonline after the event.\nGuest Speaker: Thais\nMenezes\n\n\nThais Menezes\nThais Menezes is a Ph.D.¬†student of the SFI Centre for Research\nTraining in Foundations of Data Science (https://www.data-science.ie/) program. She is based in\nIreland and her university is the University College Dublin (UCD).\nCurrently, her thesis is regarding Record Linkage and she works with Dt\nMichael Fop and Professor Brendan Murph - both also from UCD. She is\nfrom Brazil and had a bachelor‚Äôs and a master‚Äôs degree in statistics.\nShe has some previous work in the Survival Analysis field in which she\nadapted a Current Status Data model to incorporate Misclassification.\nDuring her master‚Äôs, she worked with the Singular Decomposition Value\ntheorem to find latent maps for cancers databases from different\ncountries. She also has experience working as a statistician at a bank\nand at a marketing company. She loves to work developing models and\nmethodologies that require a strong computational base and she loves to\ncode using R and Python.\nAbout the Linkage Seminar\nSeries\nThe Massive Data Institute at Georgetown University‚Äôs McCourt School\nof Public Policy co-hosts monthly Linkage Seminars with the American\nStatistical Association‚Äôs Record Linkage Interest Group. These monthly\nseminars feature interdisciplinary data experts discussing their work on\ndata linkages across data types, sectors, domains, and disciplines.\n\n\n\n",
    "preview": "posts/2022-02-16-youre-invited-linkage-seminar-series-thursday-february-24/portrait.jpg",
    "last_modified": "2022-09-06T20:16:15-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-23-record-linkage-at-the-gpsg-community-pantry/",
    "title": "Record Linkage at the Duke GPSG Community Pantry",
    "description": "The Duke Graduate and Professional Student Government (GPSG) Community Pantry is a student-operated food pantry serving the student community at Duke University. In this post, I describe the record linkage system used at the Pantry to identify individual customers and obtain their order history. This is done using a Python module for deterministic record linkage and model evaluation techniques which I describe in detail.",
    "author": [
      {
        "name": "Olivier Binette",
        "url": "https://olivierbinette.github.io"
      }
    ],
    "date": "2021-12-23",
    "categories": [
      "Full Post",
      "Python",
      "Record Linkage"
    ],
    "contents": "\n\nContents\nIntroduction\nFood Insecurity on\nCampus\n\nThe Record Linkage\nApproach\nDeterministic Record\nLinkage Rule\nImplementation\nLimitations\n\nModel Evaluation\nPairwise Precision and\nRecall\nResults\n\nFinal thoughts\nAbout the Author\n\nFigure from https://gpsg.duke.edu/resources-for-students/community-pantry/Introduction\nDuke‚Äôs Graduate and Professional Student Government (GPSG) has been\noperating a community food pantry for about five years. The pantry\nprovides nonperishable food and basic need items to graduate and\nprofessional students on campus. There is a weekly bag program, where\nstudents order customized bags of food to be picked up on Saturdays, as\nwell as an in-person shopping program open on Thursdays and\nSaturdays.\n\nFigure 1: Weekly number of customers at the Pantry. The black line is a\nmoving average of weekly visits.\n\nThe weekly bag program, which began in May 2018 and is still the most\npopular pantry offering, provides quite a bit of data regarding pantry\ncustomers and their habits. Some customers have ordered more than 80\ntimes in the past 2 years, while others only ordered once or twice. For\nevery order, we have the customer‚Äôs first name and last initial, an\nemail address (which became mandatory around mid 2018), a phone number\nin a few cases, an address in some cases (for delivery), we have\ndemographic information some cases, and we have the food order\ninformation. Available quasi-identifying information is shown in Table 1\nbelow.\nTable 1: Quasi-identifying information provided on Qualtrics\nbag order forms. Note that phone number and address were only required\nwhile delivery was offered. Furthermore, most customers stop answering\ndemographic questions after a few orders.\nQuestion no.\nQuestion\nAnswer form\nMandatory?\n-\nIP address\n-\nYes\n2\nFirst name and last initial\nFree form\nYes\n3\nDuke email\nFree form\nYes\n4\nPhone number\nFree form\nNo\n6\nAddress\nFree form\nNo\n8\nFood allergies\nFree form\nNo\n9\nNumber of members in household\n1-2 or 3+\nYes\n10\nWant baby bag?\nYes or no\nYes\n30\nDegree\nMultiple choices or Other\nNo\n31\nSchool\nMultiple choices or Other\nNo\n32\nYear in graduate school\nMultiple choices\nNo\n33\nNumber of adults in household\nMultiple choices\nNo\n34\nNumber of children in household\nMultiple choices\nNo\nGaining the most insight from this data requires linking order\nrecords from the same customer. Identifying individual customers and\nassociating them with an order history allows us to investigate shopping\nrecurrence patterns and identify potential issues with the pantry‚Äôs\noffering. For instance, we can know who stopped ordering from the pantry\nafter the home delivery program ended. These are people who, most\nlikely, do not have a car to get to the pantry but might benefit from\nnew programs, such as a ride-share program or a gift card program.\nThis blog post describes the way in which records are linked at the\nCommunity Pantry. As we will see, the record linkage problem is not\nparticularly difficult. It is not trivial either, however, and it does\nrequire care to ensure that it runs reliably and efficiently, and that\nit is intelligible and properly validated. This post goes in detail into\nthese two aspects of the problem.\nRegarding efficiency and reliability of the software system, I\ndescribe the development of a Python module, called GroupByRule,\nfor record linkage at the pantry. This Python module is maintainable,\ndocumented and tested, ensuring reliability of the system and the\npotential for its continued use throughout the years, even as technical\nvolunteers change at the pantry. Regarding validation of the record\nlinkage system, I describe simple steps that can be taken to evaluate\nmodel performance.\nBefore jumping into the technical part, let‚Äôs take a step back to\ndiscuss the issue of food insecurity on campus.\nFood Insecurity on Campus\nIt is often surprising to people that some Duke students might\nstruggle having access to food. After all, Duke is one of the richest\ncampuses in the US with its 12\nbillion endowment, high tuition and substantial research grants.\nPrior to the covid-19 pandemic, this wealth could be seen on campus and\nbenefit many. Every weekday, there were several conferences and events\nwith free food. Me and many other graduate students would participate in\nthese events, earning 3-4 free lunches every week. Free food on campus\nis now a thing of the past, for the most part.\nHowever, free lunch or not, it‚Äôs important to realize the many\nfinancial challenges which students can face. International students on\nF-1 and J-1 visas have limited employment opportunities in the US. Many\ngraduate students are married, have children or have other dependents\nwhich may not be eligible to work in the US either. Even if they are\nlucky enough to be paid a 9 or 12-month stipend, this stipend doesn‚Äôt go\nvery far. For other students, going to Duke means living on a mixture of\nloans, financial aid, financial support from parents, and side jobs. Any\nimbalance in this rigid system can leave students having to compromise\nbetween their education and their health.\nA 2019 study from the World Food Policy Center reported that about\n19% of graduate and professional students at Duke experienced food\ninsecurity in the past year. This means they were unable to afford a\nbalanced and sufficient diet, they were afraid of not having enough\nmoney for food, or they skipped meals and went hungry due to lack of\nmoney. The GPSG Community Pantry has been leading efforts to expand food\ninsecurity monitoring on campus ‚Äì we are hoping to have more data in\n2022 and in following years.\nThe Record Linkage Approach\nThe bag order form contains email addresses which are highly reliable\nfor linkage. If two records have the same email, we know for certain\nthat they are from the same customer. However, customers do not always\nenter the same email address when submitting orders. Despite the request\nto use a Duke email address, some customers use personal emails.\nFurthermore, Duke email addresses have two forms. For instance, my duke\nemail is both ob37@duke.edu and\nolivier.binette@duke.edu. Emails are therefore not\nsufficient for linkage. Phone numbers can be used as well, but these are\nonly available for the period when home delivery was available.\nFirst name and last initial can be used to supplement emails and\nphone numbers. Again, agreement on first name and last initial provides\nstrong evidence for match. On the other hand, people do not always enter\ntheir names in the same way.\nCombining the use of emails, phone numbers, and names, we may\ntherefore link records which agree on any one of these attributes. This\nis a simple deterministic record linkage approach which should be\nreliable enough for the data analysis use of the pantry.\nDeterministic Record Linkage\nRule\nTo be more precise, record linkage proceeds as follows:\nRecords are processed to clean and standardize the email, phone\nand name attributes. That is, leading and trailing whitespace are\nremoved, capitalization is standardized, phone numbers are validated and\nstandardized, and punctuation is removed from names.\nRecords which agree on any of their email, phone or name\nattributes are linked together.\nConnected components of the resulting graph are computed in order\nto obtain record clusters.\nThis record linkage procedure is extremely simple. It relies the fact\nthat all three attributes are reliable indicators of a match and that,\nfor two matching records, it is likely that at least one of these three\nattributes will be in agreement.\nAlso, the simplicity of the approach allows the use of available\nadditional information (such as IP address and additional questions) for\nmodel validation. If the use of this additional information does not\nhighlight any flaws with the simple deterministic approach, then this\nmeans that the deterministic approach is already good enough. We will\ncome back to this when discussing model validation techniques.\nImplementation\nOur deterministic record linkage system is implemented in Python with\nsome generality. The goal is for the system to be able to adapt to\nchanges in data or processes.\nThe fundamental component of the system is a LinkageRule\nclass. LinkageRule objects can be fitted to data, providing either a\nclustering or a linkage graph. For instance, a LinkageRule might be a\nrule to link all records which agree on the email attribute. Another\nLinkageRule might summarize a set of other rules, such as taking the\nunion or intersection of their links.\nThe interface is as follows:\n\nfrom abc import ABC, abstractmethod\n\n\nclass LinkageRule(ABC):\n    \"\"\"\n    Interface for a linkage rule which can be fitted to data.\n\n    This abstract class specifies three methods. The `fit()` method fits the \n    linkage rule to a pandas DataFrame. The `graph` property can be used after \n    `fit()` to obtain a graph representing the linkage fitted to data.  The \n    `groups` property can be used after `fit()` to obtain a membership vector \n    representing the clustering fitted to data.\n    \"\"\"\n    @abstractmethod\n    def fit(self, df):\n        pass\n\n    @property\n    @abstractmethod\n    def graph(self):\n        pass\n\n    @property\n    @abstractmethod\n    def groups(self):\n        pass\n\nNote that group membership vectors, our representation for cluster\ngroups, are meant to be a numpy integer array with entries indicating\nwhat group (cluster) a given record belongs to. Such a ‚Äúgroups‚Äù vector\nshould not contain NA values; rather it should contain distinct integers\nfor records that are not in the same cluster.\nWe will now define two other classes, Match and\nAny, which allow us to implement deterministic record\nlinkage. The Match class implements an exact matching rule,\nwhile Any is the logical disjunction of a given set of\nrules. Our deterministic record linkage rule for the pantry will\ntherefore be defined as follows:\n\nrule = Any(Match(\"name\"), Match(\"email\"), Match(\"phone\"))\n\nFollowing the LinkageRule interface, this rule will then\nbe fitted to the data and used as follows:\n\nrule.fit(data)\ndata.groupby(rule.groups).last() # Get last visit data for all customers.\n\nThe benefit of this general interface is that it is extendable. By\ndefault, the Any class will return connected components\nwhen requesting group clusters. However, other clustering approaches\ncould be used. Exact matching rules could also be relaxed to fuzzy\nmatching rules based on string distance metrics or probabilistic record\nlinkage. All of this can be implemented as additional\nLinkageRule subclasses in a way which is compatible with\nthe above.\nLet‚Äôs now work on the Match class. For efficiency, we‚Äôll\nwant Match to operate at the groups level. That is, if\nMatch is called on a set of rules, then we‚Äôll first compute\ngroups for these rules, before computing the intersection of these\ngroups. This core functionality is implemented in the function\n_groups_from_rules() below. The function\n_groups() is a simple wrapper to interpret strings as a\nmatching rule on the corresponding column.\n\nimport pandas as pd\nimport numpy as np\nimport itertools\nfrom igraph import Graph\n\ndef _groups(rule, df):\n    \"\"\"\n    Fit linkage rule to dataframe and return membership vector.\n\n    Parameters\n    ----------\n    rule: string or LinkageRule\n        Linkage rule to be fitted to the data. If `rule` is a string, then this \n        is interpreted as an exact matching rule for the corresponding column.\n    df: DataFrame\n        pandas Dataframe to which the rule is fitted.\n\n    Returns\n    -------\n    Membership vector (i.e. integer vector) u such that u[i] indicates the \n    cluster to which dataframe row i belongs. \n\n    Notes\n    -----\n    NA values are considered to be non-matching.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> df = pd.DataFrame({\"fname\":[\"Olivier\", \"Jean-Francois\", \"Alex\"], \n      \"lname\":[\"Binette\", \"Binette\", pd.NA]})\n\n    Groups specified by distinct first names:\n    >>> _groups(\"fname\", df)\n    array([2, 1, 0], dtype=int8)\n\n    Groups specified by same last names:\n    >>> _groups(\"lname\", df)\n    array([0, 0, 3], dtype=int8)\n\n    Groups specified by a given linkage rule:\n    >>> rule = Match(\"fname\")\n    >>> _groups(rule, df)\n    array([2, 1, 0])\n    \"\"\"\n    if (isinstance(rule, str)):\n        arr = np.array(pd.Categorical(df[rule]).codes, dtype=np.int32) # Specifying dtype avoids overflow issues\n        I = (arr == -1)  # NA value indicators\n        arr[I] = np.arange(len(arr), len(arr)+sum(I))\n        return arr\n    elif isinstance(rule, LinkageRule):\n        return rule.fit(df).groups\n    else:\n        raise NotImplementedError()\n\n\ndef _groups_from_rules(rules, df):\n    \"\"\"\n    Fit linkage rules to data and return groups corresponding to their logical \n    conjunction.\n\n    This function computes the logical conjunction of a set of rules, operating \n    at the groups level. That is, rules are fitted to the data, membership \n    vector are obtained, and then the groups specified by these membership \n    vectors are intersected.\n\n    Parameters\n    ----------\n    rules: list[LinkageRule]\n        List of strings or Linkage rule objects to be fitted to the data. \n        Strings are interpreted as exact matching rules on the corresponding \n        columns.\n\n    df: DataFrame\n        pandas DataFrame to which the rules are fitted.\n\n    Returns\n    -------\n    Membership vector representing the cluster to which each dataframe row \n    belongs.\n\n    Notes\n    -----\n    NA values are considered to be non-matching.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> df = pd.DataFrame({\"fname\":[\"Olivier\", \"Jean-Francois\", \"Alex\"], \n      \"lname\":[\"Binette\", \"Binette\", pd.NA]})\n    >>> _groups_from_rules([\"fname\", \"lname\"], df)\n    array([2, 1, 0])\n    \"\"\"\n\n    arr = np.array([_groups(rule, df) for rule in rules]).T\n    groups = np.unique(arr, axis=0, return_inverse=True)[1]\n    return groups\n\nWe can now implement Match as follows. Note that the\nGraph representation of the clustering is only computed if\nand when needed.\n\nclass Match(LinkageRule):\n    \"\"\"\n    Class representing an exact matching rule over a given set of columns.\n\n    Attributes\n    ----------\n    graph: igraph.Graph\n        Graph representing linkage fitted to the data. Defaults to None and is \n        instantiated after the `fit()` function is called.\n\n    groups: integer array\n        Membership vector for the linkage clusters fitted to the data. Defaults \n        to None and is instantiated after the `fit()` function is called.\n\n    Methods\n    -------\n    fit(df)\n        Fits linkage rule to the given dataframe.\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> df = pd.DataFrame({\"fname\":[\"Olivier\", \"Jean-Francois\", \"Alex\"], \n    \"lname\":[\"Binette\", \"Binette\", pd.NA]})\n\n    Link records which agree on both the \"fname\" and \"lname\" fields.\n    >>> rule = Match(\"fname\", \"lname\")\n\n    Fit linkage rule to the data.\n    >>> _ = rule.fit(df)\n\n    Construct deduplicated dataframe, retaining only the first record in each cluster.\n    >>> _ = df.groupby(rule.groups).first()\n    \"\"\"\n\n    def __init__(self, *args):\n        \"\"\"\n        Parameters\n        ----------\n        args: list containing strings and/or LinkageRule objects.\n            The `Match` object represents the logical conjunction of the set of \n            rules given in the `args` parameter. \n        \"\"\"\n        self.rules = args\n        self._update_graph = False\n        self.n = None\n\n    def fit(self, df):\n        self._groups = _groups_from_rules(self.rules, df)\n        self._update_graph = True\n        self.n = df.shape[0]\n\n        return self\n\n    @property\n    def groups(self):\n        return self._groups\n\nOne more method is needed to complete the implementation of a\nLinkageRule, namely the graph property. This\nproperty returns a Graph object corresponding to the matching rule. The\ngraph is built as follows. First, we construct an inverted index for the\nclustering. That is, we construct a dictionary associating to each\ncluster the nodes which it contains. Then, an edge list is obtained by\nlinking all pairs of nodes which belong to the same cluster. Note that\nthe pure Python implementation below if not efficient for large\nclusters. This is not a problem for now since we will generally avoid\ncomputing this graph.\n\n# Part of the definition of the `Match` class:\n    @property\n    def graph(self) -> Graph:\n        if self._update_graph:\n            # Inverted index\n            clust = pd.DataFrame({\"groups\": self.groups}\n                                 ).groupby(\"groups\").indices\n            self._graph = Graph(n=self.n)\n            self._graph.add_edges(itertools.chain.from_iterable(\n                itertools.combinations(c, 2) for c in clust.values()))\n            self._update_graph = False\n        return self._graph\n\nFinally, let‚Äôs implement the Any class. It‚Äôs purpose is\nto take the union (i.e.¬†logical disjunction) of a set of rules. Just\nlike for Match, we can choose to operate at the groups or\ngraph level. Here we‚Äôll work at the groups level for efficiency. That\nis, given a set of rules, Any will first compute their\ncorresponding clusters before merging overlapping clusters.\nThere are quite a few different ways to efficiently merge clusters.\nHere we‚Äôll merge clusters by computing a ‚Äúpath graph‚Äù representation,\ntaking the union of these graphs, and then computing connected\ncomponents. For a given clustering, say containing records a, b, and c,\nthe ‚Äúpath graph‚Äù links records as a path a‚Äìb‚Äìc.\nFirst, we define the functions needed to compute path graphs:\n\ndef pairwise(iterable):\n    \"\"\"\n    Iterate over consecutive pairs:\n        s -> (s[0], s[1]), (s[1], s[2]), (s[2], s[3]), ...\n\n    Note\n    ----\n    Current implementation is from itertools' recipes list available at \n    https://docs.python.org/3/library/itertools.html\n    \"\"\"\n    a, b = itertools.tee(iterable)\n    next(b, None)\n    return zip(a, b)\n\n\ndef _path_graph(rule, df):\n    \"\"\"\n    Compute path graph corresponding to the rule's clustering: cluster elements \n    are connected as a path.\n\n    Parameters\n    ----------\n    rule: string or LinkageRule\n        Linkage rule for which to compute the corresponding path graph \n        (strings are interpreted as exact matching rules for the corresponding column).\n    df: DataFrame\n        Data to which the linkage rule is fitted.\n\n    Returns\n    -------\n    Graph object such that nodes in the same cluster (according to the fitted \n    linkage rule) are connected as graph paths.\n    \"\"\"\n    gr = _groups(rule, df)\n    \n    # Inverted index\n    clust = pd.DataFrame({\"groups\": gr}\n                         ).groupby(\"groups\").indices\n    graph = Graph(n=df.shape[0])\n    graph.add_edges(itertools.chain.from_iterable(\n        pairwise(c) for c in clust.values()))\n\n    return graph\n\nWe can now implement the Any class:\n\nclass Any(LinkageRule):\n    \"\"\"\n    Class representing the logical disjunction of linkage rules.\n\n    Attributes\n    ----------\n    graph: igraph.Graph\n        Graph representing linkage fitted to the data. Defaults to None and is \n        instantiated after the `fit()` function is called.\n\n    groups: integer array\n        Membership vector for the linkage clusters fitted to the data. Defaults \n        to None and is instantiated after the `fit()` function is called.\n\n    Methods\n    -------\n    fit(df)\n        Fits linkage rule to the given dataframe.\n    \"\"\"\n\n    def __init__(self, *args):\n        \"\"\"\n        Parameters\n        ----------\n        args: list containing strings and/or LinkageRule objects.\n            The `Any` object represents the logical disjunction of the set of \n            rules given by `args`. \n        \"\"\"\n        self.rules = args\n        self._graph = None\n        self._groups = None\n        self._update_groups = False\n\n    def fit(self, df):\n        self._update_groups = True\n        graphs_vect = [_path_graph(rule, df) for rule in self.rules]\n        self._graph = igraph.union(graphs_vect)\n        return self\n\n    @property\n    def groups(self):\n        if self._update_groups:\n            self._update_groups = False\n            self._groups = np.array(\n                self._graph.clusters().membership)\n        return self._groups\n\n    @property\n    def graph(self) -> Graph:\n        return self._graph\n\nThe complete Python module (still under development) implementing\nthis approach can be found on Github at OlivierBinette/GroupByRule.\nLimitations\nThere are quite a few limitations with this simple deterministic\napproach. We‚Äôll see in the model evaluation section that these do not\naffect performance to a large degree. However, for a system used with\nmore data or over a longer timeframe, these should be carefully\nconsidered.\nFirst, the deterministic linkage does not allow the consideration of\ncontradictory evidence. For instance, if long-form Duke email addresses\nare provided on two records and do not agree\n(e.g.¬†‚Äúolivier.binette@duke.edu‚Äù and ‚Äúolivier.bonhomme@duke.edu‚Äù are\nprovided), then we know for sure that the records do not\ncorrespond to the same individual, even if the same name was provided\n(here Olivier B.). The consideration of such evidence could rely on\nprobabilistic record linkage, where each record pair is associated a\nmatch probability.\nSecond, the use of connected components to resolve transitivity can\nbe problematic, as a single spurious link could connect two large\nclusters by mistake. More sophisticated graph clustering techniques, in\ncombination with probabilistic record linkage, would be required to\nmitigate the issue.\nModel Evaluation\nI cannot share any of the data which we have at the Pantry. However,\nI can describe general steps to be taken to evaluate model performance\nin practice.\nPairwise Precision and\nRecall\nHere we will evaluate linkage performance using pairwise precision\n\\(P\\) and recall \\(R\\). The precision \\(P\\) is defined as the proportion of\npredicted links which are true matches, whereas \\(R\\) is the proportion of true matches which\nare correctly predicted. That is, if \\(TP\\) is the number of true positive links,\n\\(P\\) the number of predicted links,\nand \\(T\\) the number of true matches,\nthen we have \\[\nP = TP/P, \\quad R = TP/T.\n\\]\nEstimating Precision\nIt is helpful to express precision and recall in cluster form, where\ncluster elements are all interlinked. Let \\(C\\) be the set of true clusters and let\n\\(\\hat C\\) be the set of predicted\nclusters. For a given cluster \\(\\hat c \\in\n\\hat C\\), let \\(C \\cap \\hat c\\)\nbe the restriction of the clustering \\(C\\) to \\(\\hat\nc\\). Then we have \\[\n  P = \\frac{\\sum_{\\hat c \\in \\hat C} \\sum_{e \\in C \\cap \\hat c} {\\lvert\ne\\rvert \\choose 2} }{ \\sum_{\\hat c \\in \\hat C} {\\lvert \\hat c \\rvert\n\\choose 2}}.\n\\]\nThe denominator can be computed exactly, while the numerator can be\nestimated by randomly sampling clusters \\(\\hat\nc \\in \\hat C\\), breaking them up into true clusters \\(e \\in C \\cap \\hat c\\), and then computing\nthe sum of the combinations \\({\\lvert e\\rvert\n\\choose 2}\\). Importance sampling could be used to reduce the\nvariance of the estimator, but it does not seem necessary for the scale\nof the data which we have at the pantry, where each predicted cluster\ncan be examined quite quickly.\nIn practice, the precision estimation process can be carried out as\nfollows:\nSample predicted clusters at random (in the case of the pantry, we\ncan take all predicted clusters).\nMake a spreadsheet with all the records corresponding to the sampled\nclusters.\nSort the spreadsheet by predicted cluster ID.\nAdd a new empty column to the spreadsheet, called\n‚ÄútrueSubClusters‚Äù.\nSeparately look at each predicted cluster. If the cluster should be\nbroken up in multiple parts, use the ‚ÄútrueSubClusters‚Äù column to provide\nidentifiers for true cluster membership. Note that these identifiers do\nnot need to match across predicted clusters.\nThe spreadsheet can then be read-in and processed in a\nstraightforward way to obtain an estimated precision value.\nEstimating Recall\nEstimating recall is a bit trickier than estimating precision, but we\ncan make one assumption to simplify the process. Assume that precision\nis exactly 1, or very close to 1, so that all predicted clusters can\nroughly be taken at face value. Estimating recall then boils to the\nproblem of identifying which predicted clusters should be merged\ntogether.\nIndeed, using the same notations as above, we can write \\[\nR = \\frac{\\sum_{ c \\in  C} \\sum_{e \\in \\hat C \\cap  c} {\\lvert e\\rvert\n\\choose 2} }{ \\sum_{ c \\in  C} {\\lvert  c \\rvert \\choose 2}}.\n\\] If precision is 1, then the denominator can be computed from\nthe sizes of merged predicted clusters. On the other hand, the nominator\nsimplifies to \\(\\sum_{e \\in \\hat C}{\\lvert e\n\\rvert \\choose 2}\\) which can be computed exactly from the sizes\nof predicted clusters. In the case of the Pantry, wrongly separated\nclusters are likely to be due to small differences in names and emails.\nTherefore, we can identify clusters which should have been merged\ntogether as follows:\nMake a spreadsheet containing canonical customer records (one\nrepresentative record for each predicted individual customer).\nCreate a new empty column named ‚ÄútrueClustersA‚Äù.\nSort the spreadsheet by name.\nGo through the spreadsheet from top to bottom, looking at whether or\nnot consecutive predicted clusters should be merged together. If so,\nwrite a corresponding cluster membership ID in the ‚ÄútrueClustersA‚Äù\ncolumn.\nCreate a new empty column named ‚ÄútrueClustersB‚Äù.\nSort the spreadsheet by email\nGo through the spreadsheet from top to bottom, looking at whether or\nnot consecutive predicted clusters should be merged together. If so,\nwrite a corresponding cluster membership ID in the ‚ÄútrueClustersB‚Äù\ncolumn.\nThis process might not catch all wrongly separated clusters, but it\nis likely to find many of the errors due to different ways of writing\nnames and different email addresses. The resulting spreadsheet can then\neasily be processed to obtain an estimated recall. If we were working\nwith a larger dataset, we‚Äôd have to use further blocking to restrict our\nconsideration to a more manageable subset of the data.\nResults\nI used the above procedures to estimate precision and recall of our\nsimple deterministic approach to deduplicate the Pantry‚Äôs data. There\nwas a total of 3281 bag order records for 689 estimated customers. The\nresults are below.\nEstimated Precision: 92%\nPrecision is somewhat low due to about 3 relatively large clusters\n(around 30-50 records each) which should have been broken up in a few\nparts. 2% precision was lost due to a couple that shared a phone number,\nwhere each had about 20 order records. The vast majority of spurious\nlinks were tied to bag orders for which only the first name was provided\n(e.g.¬†‚ÄúSam‚Äù). The use of negative evidence to distinguish between\nindividuals would help resolve these cases.\nEstimated Recall: 99.6%\nThis is certainly an underestimate, but it does show that missing\nlinks are not obviously showing up. Given the structure of the Pantry\ndata, it is likely that recall is indeed quite high.\nFinal thoughts\nThere are many ways in which the record linkage approach could be\nimproved. As previously discussed, probabilistic record linkage would\nallow the consideration of negative evidence and the use of additional\nquasi-identifying information (such as IP addresses and other responses\non the bag order forms). I‚Äôm looking forward to building on the\nGroupByRule Python module to provide a user-friendly and\nunified interface to more flexible methodology.\nHowever, it is important to ensure that any record linkage approach\nis intelligible and rooted in a good understanding of the underlying\ndata. In this context, the use of a well-thought deterministic approach\ncan provide good performance, at least as a first step or baseline for\ncomparison. Furthermore, it is important to spend sufficient time\ninvestigating the results of the linkage to evaluate performance. I have\nhighlighted simple steps which can be taken to estimate precision and\nmake a good effort at identifying missing links. This is highly\ninformative for model validation, improvement, and for the\ninterpretation of any following results.\nAbout the Author\nOlivier Binette is a PhD Candidate in the\nStatistical Science Department at Duke University. He works part-time as\nresearch coordinator at the GPSG Community Pantry.\n\n\n\nCampbell, Kevin M., Dennis Deck, and Antoinette Krupski. 2008.\n‚ÄúRecord Linkage Software in the Public Domain: A Comparison of\nLink Plus, the Link King, and a\n‚ÄôBasic‚Äô Deterministic Algorithm.‚Äù Health Informatics\nJournal 14 (1): 5‚Äì15.\n\n\nGomatam, Shanti, Randy Carter, Mario Ariet, and Glenn Mitchell. 2002.\n‚ÄúAn Empirical Comparison of Record Linkage Procedures.‚Äù\nStatistics in Medicine 21 (10): 1485‚Äì96. https://doi.org/10.1002/sim.1147.\n\n\nMonge, Alvaro E., and Charles P. Elkan. 1997. ‚ÄúAn Efficient\nDomain-Independent Algorithm for Detecting Approximately Duplicate\nDatabase Records.‚Äù Proceedings of the SIGMOD 1997 Workshop on\nResearch Issues on Sata Mining and Knowledge Discovery, 23‚Äì29. https://doi.org/10.1.1.28.8405.\n\n\nPotosky, Arnold L., Gerald F. Riley, James D. Lubitz, Renee M. Mentnech,\nand Larry G. Kessler. 1993. ‚ÄúPotential for Cancer Related Health\nServices Research Using a Linked Medicare-Tumor Registry\nDatabase.‚Äù Medical Care 31 (8): 732‚Äì48. https://doi.org/10.1097/00005650-199308000-00006.\n\n\nTromp, Miranda, Anita C. Ravelli, Gouke J. Bonsel, Arie Hasman, and\nJohannes B. Reitsma. 2011. ‚ÄúResults from Simulated Data Sets:\nProbabilistic Record Linkage Outperforms Deterministic Record\nLinkage.‚Äù Journal of Clinical Epidemiology 64 (5):\n565‚Äì72. https://doi.org/10.1016/j.jclinepi.2010.05.008.\n\n\n\n\n",
    "preview": "http://dukegpsc.org/wp-content/uploads/2019/07/pantry.jpg",
    "last_modified": "2022-09-06T20:16:15-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-12-20-welcome/",
    "title": "Welcome!",
    "description": "Welcome to the blog of ASA's Record Linkage Interest Group.",
    "author": [
      {
        "name": "Olivier Binette",
        "url": "https://olivierbinette.github.io"
      }
    ],
    "date": "2021-12-20",
    "categories": [],
    "contents": "\n\nContents\nHow to Contribute\nCode of Conduct\n\nWelcome! This is the blog of the American Statistical Association‚Äôs\nRecord Linkage Interest Group (RLIG).\nIf you are interested in deterministic and probabilistic record\nlinkage, entity resolution, data fusion, or statistical matching, then\nthis is for you. We hope you become a member of RLIG (it‚Äôs free) to stay\nup to date with news and events in our community. You can learn more on\nour main website at https://sites.google.com/view/rlig/.\nWe hope this blog will help foster community within RLIG. We\nencourage everyone to contribute. Posts can be short, such as to share\ninteresting papers and associated thoughts, or they can be longer to go\ninto any topic you find interesting. You can comment on posts to get in\ntouch with their authors.\nFirst and foremost, we want this blog to be a welcoming place for\nall. There is no ‚Äúright‚Äù way to write posts ‚Äì express yourself as you\nlike. We want to hear your unique perspective. Reply to comments,\nwelcome feedback, and make connections!\nInstructions to write blog posts are available on the Contribute\npage. It might seem a bit overwhelming if you have not worked with these\ntools before. However, we are here to help. If you‚Äôd like to post\nsomething but don‚Äôt want to use R, we‚Äôll create the post for you. If\nyou‚Äôre unsure about any of the steps below, get in touch and we‚Äôll\nprovide support. Contact me, Olivier Binette, for any\nquestion. I‚Äôll be more than happy to help you contribute to the\nblog.\nHow to Contribute\nSee the Contribute\npage for information on how to contribute to this blog.\nCode of Conduct\nPlease note that the RecordLinkageIG.github.io project is released\nwith a Contributor\nCode of Conduct. By contributing to this project, you agree to abide\nby its terms.\n\n\n\n",
    "preview": "posts/2021-12-20-welcome/wave.png",
    "last_modified": "2022-09-06T20:16:15-04:00",
    "input_file": {},
    "preview_width": 512,
    "preview_height": 512
  }
]
